{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vanishing gradient problem: cumulative back-propagated error signals either shrink rapidly, or grow out of bounds. They decay exponentially in the number of layers, or they explode. The result is that the final trained network converges to a poor local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation (non-linear) functions that do not saturate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Rectifier Linear Unit, ReLU: $y=max(0,x)$, $y \\in [0,\\infty]$, learning rate $\\alpha \\rightarrow 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leaky ReLU: $y=max(sÂ·x,x)$ , typically $s=0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential Linear Unit, ELU: $y=s(e^{x}-1$), usually $s=1$. if $s=1$, then $y \\in [-1,\\infty]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same function as previous examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"notMNIST.pickle\"\n",
    "def make_datasets (file, n_training_samples=0, n_dev_samples=0, \n",
    "                   n_testing_samples=0, one_hot=False):\n",
    "    with open (file,'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "        f.close\n",
    "\n",
    "    train_dataset = dataset['train_dataset']\n",
    "    train_labels = dataset['train_labels']\n",
    "    dev_dataset = dataset['valid_dataset']\n",
    "    dev_labels = dataset['valid_labels']\n",
    "    test_dataset = dataset['test_dataset']\n",
    "    test_labels = dataset['test_labels']\n",
    "\n",
    "    #Prepare training, dev (validation) and final testing data. \n",
    "    #It has to be reshaped since (n_samples, n_fatures) are expected\n",
    "\n",
    "    all_training_samples, width, height = train_dataset.shape\n",
    "    train_attributes = np.reshape(train_dataset, (all_training_samples, \n",
    "                                                  width * height))\n",
    "    if (n_training_samples != 0):\n",
    "        train_attributes = train_attributes[0:n_training_samples]\n",
    "        train_labels = train_labels[0:n_training_samples]\n",
    "\n",
    "    all_dev_samples, width, height = dev_dataset.shape\n",
    "    dev_attributes = np.reshape(dev_dataset,\n",
    "                                       (all_dev_samples, width * height))\n",
    "    if (n_dev_samples != 0):\n",
    "        dev_attributes = dev_attributes[0:n_dev_samples]\n",
    "        dev_labels = dev_labels[0:n_dev_samples]\n",
    "\n",
    "    all_testing_samples, width, height = test_dataset.shape\n",
    "    test_attributes = np.reshape(test_dataset, (all_testing_samples, width * height))\n",
    "    if (n_testing_samples != 0):\n",
    "        test_attributes = test_attributes[0:n_testing_samples]\n",
    "        test_labels = test_labels[0:n_testing_samples]\n",
    "\n",
    "    # If one-hot encoding is requested, then funtion OneHotEcoding \n",
    "    # from SciKit-Learn is called    \n",
    "    if one_hot:\n",
    "        enc = OneHotEncoder(sparse=False)\n",
    "        # Labels are one-dimensional vectors, \n",
    "        # and are reshaped to matrices of one column\n",
    "        train_labels = enc.fit_transform(train_labels.reshape(len(train_labels),1))\n",
    "        dev_labels = enc.fit_transform(dev_labels.reshape(len(dev_labels), 1))\n",
    "        test_labels = enc.fit_transform(test_labels.reshape(len(test_labels), 1))\n",
    "\n",
    "    return (train_attributes, train_labels, dev_attributes, \n",
    "            dev_labels, test_attributes, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAINING_SAMPLES = 10000\n",
    "NUM_DEV_SAMPLES = 1000\n",
    "NUM_TESTING_SAMPLES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_dev, y_dev, x_test, y_test = make_datasets(file_name, \n",
    "                                 n_training_samples=NUM_TRAINING_SAMPLES,\n",
    "                                 n_dev_samples=NUM_DEV_SAMPLES, \n",
    "                                 n_testing_samples=NUM_TESTING_SAMPLES,\n",
    "                                 one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the 28x28-300-200-100-10 deep neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-paramenters configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "epochs_to_display = 200\n",
    "batch_size = 200\n",
    "learning_rate = 0.01\n",
    "\n",
    "n_inputs = len(x_train[0])\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 200\n",
    "n_hidden3 = 100\n",
    "n_outputs = len(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the input __X__ and target __t__ matrices are defined as placeholders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"io\"):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None,n_inputs), name=\"X\")\n",
    "    t = tf.placeholder(dtype=tf.float32, shape=(None,n_outputs), name=\"t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the neural network topology is defined: A full-connected 28x28-300-200-100-10 deep neural network. Note that ReLU is the activation function for the hidden layers, and linear logits with softmax for the output. net_out represents the logits of the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    net_out = tf.layers.dense(hidden3, n_outputs, name=\"net_out\")\n",
    "    y = tf.nn.softmax(logits=net_out, name=\"y\")\n",
    "    rounded_y = tf.round(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and cost functions with cross entropy and log-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-b4b229523c05>:2: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=t, logits=net_out)\n",
    "    mean_log_loss = tf.reduce_mean(cross_entropy, name=\"mean_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the learning algorithm: gradient descent with back-prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(mean_log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(y, 1), tf.argmax(t, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions,tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 Train accuracy:  0.6782 Development accuracy:  0.674\n",
      "400 Train accuracy:  0.7992 Development accuracy:  0.799\n",
      "600 Train accuracy:  0.8182 Development accuracy:  0.811\n",
      "800 Train accuracy:  0.8297 Development accuracy:  0.812\n",
      "1000 Train accuracy:  0.8374 Development accuracy:  0.821\n",
      "1200 Train accuracy:  0.8439 Development accuracy:  0.83\n",
      "1400 Train accuracy:  0.8486 Development accuracy:  0.828\n",
      "1600 Train accuracy:  0.8505 Development accuracy:  0.831\n",
      "1800 Train accuracy:  0.8574 Development accuracy:  0.83\n",
      "2000 Train accuracy:  0.8642 Development accuracy:  0.831\n",
      "2200 Train accuracy:  0.8678 Development accuracy:  0.832\n",
      "2400 Train accuracy:  0.8733 Development accuracy:  0.835\n",
      "2600 Train accuracy:  0.8773 Development accuracy:  0.836\n",
      "2800 Train accuracy:  0.8795 Development accuracy:  0.839\n",
      "3000 Train accuracy:  0.8782 Development accuracy:  0.834\n",
      "3200 Train accuracy:  0.8863 Development accuracy:  0.837\n",
      "3400 Train accuracy:  0.8918 Development accuracy:  0.836\n",
      "3600 Train accuracy:  0.8954 Development accuracy:  0.837\n",
      "3800 Train accuracy:  0.8992 Development accuracy:  0.835\n",
      "4000 Train accuracy:  0.9027 Development accuracy:  0.836\n",
      "4200 Train accuracy:  0.9059 Development accuracy:  0.836\n",
      "4400 Train accuracy:  0.9024 Development accuracy:  0.839\n",
      "4600 Train accuracy:  0.9118 Development accuracy:  0.837\n",
      "4800 Train accuracy:  0.9164 Development accuracy:  0.836\n",
      "5000 Train accuracy:  0.9199 Development accuracy:  0.834\n",
      "5200 Train accuracy:  0.9256 Development accuracy:  0.836\n",
      "5400 Train accuracy:  0.9289 Development accuracy:  0.836\n",
      "5600 Train accuracy:  0.933 Development accuracy:  0.839\n",
      "5800 Train accuracy:  0.9262 Development accuracy:  0.834\n",
      "6000 Train accuracy:  0.9367 Development accuracy:  0.833\n",
      "6200 Train accuracy:  0.9418 Development accuracy:  0.83\n",
      "6400 Train accuracy:  0.9464 Development accuracy:  0.833\n",
      "6600 Train accuracy:  0.9506 Development accuracy:  0.83\n",
      "6800 Train accuracy:  0.9534 Development accuracy:  0.829\n",
      "7000 Train accuracy:  0.9572 Development accuracy:  0.832\n",
      "7200 Train accuracy:  0.9478 Development accuracy:  0.833\n",
      "7400 Train accuracy:  0.9589 Development accuracy:  0.829\n",
      "7600 Train accuracy:  0.9621 Development accuracy:  0.83\n",
      "7800 Train accuracy:  0.9652 Development accuracy:  0.828\n",
      "8000 Train accuracy:  0.9683 Development accuracy:  0.833\n",
      "8200 Train accuracy:  0.9706 Development accuracy:  0.833\n",
      "8400 Train accuracy:  0.9726 Development accuracy:  0.831\n",
      "8600 Train accuracy:  0.9665 Development accuracy:  0.836\n",
      "8800 Train accuracy:  0.9742 Development accuracy:  0.831\n",
      "9000 Train accuracy:  0.9763 Development accuracy:  0.834\n",
      "9200 Train accuracy:  0.9785 Development accuracy:  0.832\n",
      "9400 Train accuracy:  0.9804 Development accuracy:  0.831\n",
      "9600 Train accuracy:  0.9823 Development accuracy:  0.829\n",
      "9800 Train accuracy:  0.9834 Development accuracy:  0.831\n",
      "10000 Train accuracy:  0.9742 Development accuracy:  0.831\n",
      "Test accuracy:  0.906\n",
      "Target values:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] \n",
      "Computed values:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "First 10 Predictions:  [ True False  True  True  True  True  True  True  True False]\n",
      "Elapsed time:  64.97738742828369 secs.\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range (int(n_epochs / epochs_to_display)):\n",
    "        for iteration in range (epochs_to_display):\n",
    "            offset = (iteration * epoch * batch_size) % (y_train.shape[0] - batch_size)\n",
    "            sess.run(train_step, feed_dict={X: x_train[offset:(offset+batch_size),:],\n",
    "                                            t: y_train[offset:(offset+batch_size),:]})\n",
    "        accuracy_train = accuracy.eval(feed_dict={X: x_train, t: y_train})\n",
    "        accuracy_dev = accuracy.eval(feed_dict={X: x_dev, t: y_dev})\n",
    "        print((epoch+1)*epochs_to_display, \"Train accuracy: \", accuracy_train, \n",
    "              \"Development accuracy: \", accuracy_dev)\n",
    "\n",
    "    accuracy_test = accuracy.eval(feed_dict={X: x_test, t: y_test})\n",
    "    print (\"Test accuracy: \", accuracy_test)\n",
    "    print (\"Target values:\\n\", y_test[0:10], \"\\nComputed values:\\n\", \n",
    "           rounded_y.eval(feed_dict={X: x_test[0:10]}))\n",
    "    print (\"First 10 Predictions: \", \n",
    "           correct_predictions.eval(feed_dict={X: x_test[0:10], t: y_test[0:10]}))\n",
    "print (\"Elapsed time: \", time()-start_time, \"secs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Results with respect to one-hidden layer model:__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train: $85\\% \\rightarrow 97\\%$; Development: $81\\% \\rightarrow 84\\%$; Final test: $87\\% \\rightarrow 90\\%$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This deep model involves a total of __600 neurons__ distributed across three hidden layers, __instead of 1,000__ neurons in the just one-hidden layer model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
